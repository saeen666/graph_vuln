{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37fe3984",
   "metadata": {},
   "source": [
    "## 1.0 - The first task takes the given dataset and train a word2vec model on it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe4e0d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib, glob, time, subprocess, re, pygraphviz, json, torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import convert\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from src.utils.functions.parse import tokenizer\n",
    "import src.data as data_util\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from halo import HaloNotebook as Halo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90237c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#location to csv file, in our case it is MSR_data\n",
    "new_data  = pd.read_csv('msr/MSR_data_cleaned.csv')\n",
    "new_data_source = pd.DataFrame(data={'func':pd.concat([new_data['func_before'], new_data['func_after']])}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "spinner = Halo(text='Tokenizing Code', spinner='dots')\n",
    "spinner.start()\n",
    "tokens_dataset = data_util.tokenize(new_data_source)\n",
    "spinner.succeed()\n",
    "\n",
    "# spinner.start(\"Training Word2Vec\")\n",
    "# w2vmodel = Word2Vec(sentences=tokens_dataset.tokens, vector_size=200, window=5, min_count=1, workers=8, epochs=100)\n",
    "# print('Saving word2vec model')\n",
    "# w2vmodel.save('msr/all_w2v_model_devign_tokens.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abf0916",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tokenizing source code')\n",
    "tokens_dataset = data_util.tokenize(new_data_source)\n",
    "\n",
    "#you can change here that instead of training word2vec again load the available dataset\n",
    "print('Training word2vec on tokens')\n",
    "w2vmodel = Word2Vec(sentences=tokens_dataset.tokens, vector_size=200, window=5, min_count=1, workers=4, epochs=10)\n",
    "print('Saving word2vec model')\n",
    "w2vmodel.save('msr/all_w2v_model_devign_tokens.model')\n",
    "#load it using: Word2Vec.load('msr/all_w2v_model_devign_tokens.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d21b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 11000\n",
    "new_data_pos = new_data[new_data['vul']==1]['func_before']\n",
    "new_data_neg = new_data[new_data['vul']==0]['func_after'].sample(sample_size)\n",
    "# print(new_data_pos.shape)\n",
    "\n",
    "from pathlib import Path\n",
    "curr_path = \"new_dataset_update/positive\" \n",
    "Path(curr_path).mkdir(parents=True, exist_ok=True)\n",
    "curr_path = \"new_dataset_update/negative\"\n",
    "Path(curr_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#writing the code from pandas to individual .c files\n",
    "for key, c_code in new_data_pos.items():\n",
    "    with open('new_dataset_update/positive/{}.c'.format(key), 'w') as f:\n",
    "        f.write(c_code)\n",
    "        \n",
    "for key, c_code in new_data_neg.items():\n",
    "    with open('new_dataset_update/negative/{}.c'.format(key), 'w') as f:\n",
    "        f.write(c_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e3dbb8",
   "metadata": {},
   "source": [
    "## 2.0 - The following code convert the C-files in the given directory to cfg representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d21bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the given three variable according to your environment\n",
    "graph_type = 'cfg'\n",
    "path_code = pathlib.Path('new_dataset_update/')   #where c code is present in the folder                    \n",
    "joern_path = '/home/ammar/vuln_code/joern-cli/'   #local path where joern-cli is present\n",
    "\n",
    "# iterate over each group/project\n",
    "for folder in tqdm(glob.glob('{}/*'.format(path_code))): #the group/folders in the directory are\n",
    "                                                            #negative and positive in this case\n",
    "    if graph_type not in folder and Path(folder).is_dir():\n",
    "        print(folder)\n",
    "        path_group = pathlib.Path('{}_{}'.format(folder,graph_type))         #make folder for the dataset\n",
    "        path_group.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "        start_time = time.time()\n",
    "\n",
    "        project = glob.glob(folder+'/*')                  #get all .c files we just generated\n",
    "\n",
    "        for path_src in tqdm(project):\n",
    "            src_file      = str(Path(path_src).resolve())\n",
    "            out_path      = str(Path(str(path_group)+'/'+path_src.split('/')[-1].replace('.c','')).resolve())+\"/\"\n",
    "            file_dir      = pathlib.Path(str(Path(str(path_group)+'/'+path_src.split('/')[-1].replace('.c','')).resolve())+\"/\")         #make folder for the dataset\n",
    "            file_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            cpg_path_cmd  = [joern_path+\"./joern-parse\",src_file,\"--out\",out_path+src_file.split(\"/\")[-1].replace(\".c\",\"\")+\".cpg\"]\n",
    "            graph_out_cmd = [joern_path+\"joern-export\",out_path+src_file.split(\"/\")[-1].replace(\".c\",\"\")+'.cpg',\"--repr\",graph_type,'--out',out_path+'/cfg/']\n",
    "            cpg_check     = Path(out_path+src_file.split(\"/\")[-1].replace(\".c\",\"\")+\".cpg\").is_file()\n",
    "            \n",
    "            pth = pathlib.Path(str(Path(str(path_group)+'/'+path_src.split('/')[-1].replace('.c','')).resolve())+\"/\")\n",
    "            if cpg_check and not Path(out_path+'cfg/').is_dir():\n",
    "                result = subprocess.call(graph_out_cmd,stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL)\n",
    "\n",
    "            # check if CFG and both CPG are present if true nothing will be generated\n",
    "            elif cpg_check and Path(out_path+'cfg/').is_dir():\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "            #if both of them all false compute CPG and generate CFG \n",
    "                result = subprocess.call(cpg_path_cmd,stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL)\n",
    "\n",
    "                result1 = subprocess.call(graph_out_cmd, stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL)\n",
    "                \n",
    "            with open(out_path+pth.name+'.c', 'w') as f, open(src_file, 'r') as f1:\n",
    "                f.write(f1.read())\n",
    "\n",
    "        print(\"--- %s miuntes ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6869481",
   "metadata": {},
   "source": [
    "## 3.0 - Converting the CFG representation into networkx graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce83ff2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67921186ba5c43bd85788a78ea4efd41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d81c4716e5d94356a5af13dbb3d1024b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "total files:   0%|          | 0/10900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0c8852774b55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mdot_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg_folder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0mdot_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mdot_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdot_arr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_empty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# removes empty graphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "check_empty  ='digraph .*\\{ *\\n*\\}' #to check if generated CFG is empty or not\n",
    "pd_list = []\n",
    "dict_e = {}\n",
    "path_code = 'new_dataset_update' #I have changed the name\n",
    "project_path = glob.glob(path_code+\"/*_cfg\")\n",
    "project_path = [i for i in project_path if 'positive' in i or 'negative' in i]\n",
    "for i in tqdm(project_path):\n",
    "    path = pathlib.PurePath(i)\n",
    "    project_name = path.name \n",
    "    project = glob.glob(i+'/*')\n",
    "    dict_e[i+' total'] = len(project)\n",
    "    dict_e[i+' removed'] = 0\n",
    "    for path_src in tqdm(project, 'total files'):\n",
    "        #print(path_src)\n",
    "        new_dict   = dict()\n",
    "        src_file   = str(Path(path_src+'/'+pathlib.PurePath(path_src).name+'.c').resolve())\n",
    "        cfg_folder = str(Path(path_src+'/'+'cfg/').resolve())\n",
    "        index      = int(pathlib.PurePath(src_file).name.replace(\".c\",\"\"))\n",
    "        target     = project_name.split('_')[0]\n",
    "        #src_file   =  str(Path(os.path.join('', *[path_code,target,str(index)+'.c'])).resolve()) #changed for new_Dataset\n",
    "        #print('{}- {}- {}- {}- {}'.format(path_src, cfg_folder,index,target,src_file))\n",
    "        with open(src_file, 'r') as f:\n",
    "            src_code = f.read()\n",
    "        dot_arr = []\n",
    "        for file in glob.glob(cfg_folder+\"/*\"):\n",
    "            with open(file,'r') as f:\n",
    "                dot_arr.append(f.read())\n",
    "        dot_arr = [x for x in dot_arr if not re.search(check_empty, x)] # removes empty graphs\n",
    "        if (len(dot_arr) == 1):                                         #if graph is not empty and is connected\n",
    "            is_connected = True\n",
    "            #G = nx.Graph(nx.drawing.nx_pydot.read_dot(Path(cfg_folder).joinpath(\"0-cfg.dot\")))\n",
    "            try:\n",
    "                with open(Path(cfg_folder).joinpath(\"0-cfg.dot\")) as f:\n",
    "                    dotFormat = f.read()\n",
    "                new_str = dotFormat.replace('\\\\\"', '')                       #To catch escape characters\n",
    "                new_str = \"\\n\".join([f_str.strip() for f_str in dotFormat.split('\\n')])\n",
    "                G = nx.drawing.nx_agraph.from_agraph(pygraphviz.AGraph(new_str)); #convert graph into Networkx object\n",
    "                new_dict['index']         = index\n",
    "                new_dict['project']       = project_name\n",
    "                new_dict['func_code']     = src_code\n",
    "                new_dict['graph']         = G\n",
    "                new_dict['is_connected']  = is_connected\n",
    "                new_dict['dot_string']    = new_str\n",
    "                if target=='positive':\n",
    "                    new_dict['target']        = 1\n",
    "                elif target =='negative':\n",
    "                    new_dict['target']        = 0\n",
    "                else:\n",
    "                    print('class other than negative and positive found')\n",
    "                pd_list.append(new_dict)\n",
    "            except ValueError as e:\n",
    "                dict_e[i+' removed'] += 1\n",
    "                pass\n",
    "            \n",
    "        dict_e[i+' used'] = dict_e[i+' total'] - dict_e[i+' removed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121b9cdc",
   "metadata": {},
   "source": [
    "## 3.1 - Displays basic stats of the graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-railway",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(json.dumps(dict_e, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ba6545",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(dict_e, indent=4))\n",
    "data = pd.DataFrame(pd_list)\n",
    "data = data[['target', 'project', 'graph','func_code','index']]\n",
    "data = data.rename(columns={'func_code': 'func'})\n",
    "print('tokenizing code')\n",
    "tokens_dataset = data_util.tokenize(data)\n",
    "\n",
    "print('Training word2vec on tokens')\n",
    "w2vmodel = Word2Vec(sentences=tokens_dataset.tokens, vector_size=200, window=5, min_count=1, workers=4, epochs=10)\n",
    "print('Converting tokens to word2vec representation')\n",
    "w2vmodel.save('msr/all_w2v_model_devign_tokens.model')\n",
    "\n",
    "w2vmodel = Word2Vec.load('new_dataset_update/word2vec.model')\n",
    "\n",
    "print(\"No of samples in dataset: {} \".format(len(data)))\n",
    "print('Complete dataset stats')\n",
    "print('\\n'*3,'Complete dataset stats\\n',\"*\"*40,'\\n')\n",
    "\n",
    "node_size_group = data.apply(lambda g: nx.number_of_nodes(g.graph),axis=1).describe()[['min', 'max','mean','std']]\n",
    "\n",
    "print(tabulate(node_size_group.to_frame(),\n",
    "               tablefmt=\"grid\", stralign='left', numalign='left',\n",
    "               headers=['Node stats']))\n",
    "\n",
    "\n",
    "print('\\n'*3)\n",
    "\n",
    "edge_size_group = data.apply(lambda g: nx.number_of_edges(g.graph),axis=1).describe()[['min', 'max','mean','std']]\n",
    "print(tabulate(edge_size_group.to_frame(),\n",
    "                   tablefmt=\"grid\", stralign='left', numalign='left',\n",
    "                   headers=['Edge stats']))\n",
    "\n",
    "print('\\n\\n')\n",
    "print('Stats for each class')\n",
    "print(\"1 = Vulnerable, 0 = Not Vulnerable\")\n",
    "print('\\n')\n",
    "for name, group in data.groupby('target'):\n",
    "    \n",
    "    edge_size_group = group.apply(lambda g: nx.number_of_edges(g.graph),axis=1).describe()[['min', 'max','mean','std']]\n",
    "    node_size_group = group.apply(lambda g: nx.number_of_nodes(g.graph),axis=1).describe()[['min', 'max','mean','std']]\n",
    "    \n",
    "    print(tabulate(node_size_group.to_frame(),\n",
    "               tablefmt=\"grid\", stralign='left', numalign='left',\n",
    "               headers=['Class {} Node stats'.format(name)]))\n",
    "    \n",
    "    print('\\n'*3)\n",
    "    print(tabulate(edge_size_group.to_frame(),\n",
    "                   tablefmt=\"grid\", stralign='left', numalign='left',\n",
    "                   headers=['Class {} Edge stats'.format(name)]))\n",
    "    print('\\n'*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f109ed7",
   "metadata": {},
   "source": [
    "## 3.2 - Saves the dataset in pytorch geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-valley",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodesEmbedding:\n",
    "    def __init__(self, nodes_dim: int, w2v_keyed_vectors: Word2VecKeyedVectors):\n",
    "        self.w2v_keyed_vectors = w2v_keyed_vectors\n",
    "        self.kv_size = w2v_keyed_vectors.vector_size\n",
    "        self.nodes_dim = nodes_dim\n",
    "\n",
    "        assert self.nodes_dim >= 0\n",
    "\n",
    "        # Buffer for embeddings with padding\n",
    "        self.target = torch.zeros(self.nodes_dim, self.kv_size).float()\n",
    "\n",
    "    def __call__(self, nodes):\n",
    "        embedded_nodes = self.embed_nodes(nodes)\n",
    "\n",
    "        nodes_tensor = torch.from_numpy(embedded_nodes).float()\n",
    "\n",
    "        self.target[:nodes_tensor.size(0), :] = nodes_tensor\n",
    "\n",
    "        return self.target\n",
    "\n",
    "    def embed_nodes(self, G):\n",
    "        embeddings = []\n",
    "\n",
    "        for (n,d) in G.nodes(data=True):\n",
    "            # Get node's code\n",
    "            node_code = d\n",
    "            # Tokenize the code\n",
    "            tokenized_code = tokenizer(\"\".join(d.values()))\n",
    "            if not tokenized_code:\n",
    "                # print(f\"Dropped node {node}: tokenized code is empty.\")\n",
    "                msg = f\"Empty TOKENIZED from node CODE {node_code}\"\n",
    "                print(msg)\n",
    "            # Get each token's learned embedding vector\n",
    "            vectorized_code = np.array(self.get_vectors(tokenized_code))\n",
    "            # The node's source embedding is the average of it's embedded tokens\n",
    "            source_embedding = np.mean(vectorized_code, 0)\n",
    "            # The node representation is the concatenation of label and source embeddings\n",
    "            #embedding = np.concatenate((np.array([node.type]), source_embedding), axis=0)\n",
    "            embeddings.append(source_embedding)\n",
    "        # print(node.label, node.properties.properties.get(\"METHOD_FULL_NAME\"))\n",
    "\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    # fromTokenToVectors\n",
    "    def get_vectors(self, tokenized_code):\n",
    "        vectors = []\n",
    "        for token in tokenized_code:\n",
    "            if token in self.w2v_keyed_vectors.key_to_index:\n",
    "                vectors.append(self.w2v_keyed_vectors[token])\n",
    "            else:\n",
    "                # print(node.label, token, node.get_code(), tokenized_code)\n",
    "                vectors.append(np.zeros(self.kv_size))\n",
    "        return vectors\n",
    "\n",
    "\n",
    "\n",
    "def nodes_to_input(G, target, nodes_dim, keyed_vectors):\n",
    "    nodes_embedding = NodesEmbedding(nodes_dim, keyed_vectors)\n",
    "    edge_index, edge_attr = convert.from_scipy_sparse_matrix(nx.adjacency_matrix(G))\n",
    "    label = torch.tensor([target]).float()\n",
    "\n",
    "    return Data(x=nodes_embedding(G), edge_index=edge_index, edge_attr=edge_attr ,y=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca102f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"input\"] = data.apply(lambda row: nodes_to_input(row.graph, row.target, nx.number_of_nodes(row.graph),\n",
    "                                                                                    w2vmodel.wv), axis=1)\n",
    "print('Writing to file/pandas')\n",
    "pd.to_pickle(data[['input','target']], 'new_dataset_update/data_random.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-arbor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
