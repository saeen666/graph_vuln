{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adult-encyclopedia",
   "metadata": {},
   "source": [
    "### Make sure following packages are installed\n",
    "- Python\n",
    "    - pandas\n",
    "    - numpy\n",
    "    - tqdm\n",
    "    - networkx\n",
    "    - pygraphviz\n",
    "    - scipy\n",
    "    - sklearn\n",
    "    - spektral\n",
    "    - tensorflow\n",
    "    - tokenizer_c (tokenizer code)\n",
    "- System\n",
    "    - Joern\n",
    "    - graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-digit",
   "metadata": {},
   "source": [
    " ## 1. Generate CFG using the following method\n",
    "- make dir for each project i.e. in our case it is will be two folders one for FFmpeg and other one is for qemu defined by variable path_code\n",
    "- define your path where joern is installed using variable joern_path\n",
    "- read the dataset and Extract all instances of source code and group them based on their project\n",
    "- iterate over each project group and save the source code in a .c file. The name of the file is the index where the source file is saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-breakfast",
   "metadata": {},
   "source": [
    "### 1.1. Save source code from JSON file to individual .c file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "national-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, glob, time, re, tokenizer_c, subprocess\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm_notebook import tqdm\n",
    "import networkx as nx\n",
    "import pygraphviz\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from spektral.data import Dataset, DisjointLoader, Graph\n",
    "from spektral.data import DisjointLoader\n",
    "from spektral.layers import GINConv, GlobalAvgPool\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = 'function.json'\n",
    "path_code = pathlib.Path('code')                         # creating path 'code' in which all the cfg and c files will be placed\n",
    "path_code.mkdir(parents=True, exist_ok=True)             #this make sure if the folder already exist do not overwrite it\n",
    "joern_path  = str(Path.home())+\"/bin/joern/joern-cli/\"   #joern path\n",
    "pd_code  = pd.read_json(dataset_file)                    #read dataset available as JSON\n",
    "#pd_code  = pd_code[:100]\n",
    "pd_group = pd_code.groupby('project')\n",
    "\n",
    "# iterate over each group/project\n",
    "for group_name, df_group in pd_group:\n",
    "    path_group = pathlib.Path(str(path_code)+'/'+group_name)         #make folder for the dataset\n",
    "    path_group.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for row_index, row in df_group.iterrows():\n",
    "        path_src = pathlib.Path(str(path_group)+'/'+str(row_index)) #make folder for a every single source code\n",
    "        path_src.mkdir(parents=True, exist_ok=True)\n",
    "        code = row['func']                                              #'func' is the attribute where whole code is \n",
    "                                                                        #present as a single function\n",
    "        file_name = str(path_group)+'/'+str(row_index)+'/'+str(row_index)+'.c'\n",
    "        f = open(file_name, 'wb')\n",
    "        f.write(code.encode())\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-execution",
   "metadata": {},
   "source": [
    "### 1.2. Extract CFG from the .c files using Joern\n",
    "- The Joern generate CFG from source code is two steps'\n",
    "    1. Export the CPG file from the source code\n",
    "    2. Parse the extracted CPG file in any supported representation: AST, CFG, PDG, CPG, etc.\n",
    "- We have saved CPGs for the entire dataset and now when extracting any required graph representation we only have to run step 2\n",
    "- The parallelization doesn't work that great with joern. To speed up the process run Joern on each separate group\n",
    "- The Entire process takes around 2-3 Hours on sir's machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "project_path = glob.glob(str(path_code)+\"/*\")   #get all the projects in the dataset\n",
    "\n",
    "for i in project_path:\n",
    "    project = glob.glob(i+'/*')                  #get all .c files we just generated\n",
    "\n",
    "    for path_src in tqdm(project):\n",
    "        src_file = str(Path(path_src+'/'+path_src.split(\"/\")[-1]+'.c').resolve())\n",
    "        out_path = str(Path(path_src+'/').resolve())\n",
    "        \n",
    "        # it checks if the CPG file is available but CFG is not available as folder\n",
    "        # devnull allows the joern to write nothing on command prompt as it can slow the parsing process\n",
    "        if Path(src_file.replace('.c','.cpg')).is_file() and not Path(str(Path(path_src+'/'+'cfg').resolve())).is_dir():\n",
    "            result = subprocess.call([joern_path+\"./joern-export\",src_file.replace('.c','.cpg'),\"--repr\",\"cfg\",'--out',out_path+'/cfg/'],\n",
    "                                 stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL)\n",
    "            \n",
    "        # check if CFG and both CPG are present if true nothing will be generated\n",
    "        elif Path(src_file.replace('.c','.cpg')).is_file() and Path(str(Path(path_src+'/'+'cfg').resolve())).is_dir():\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "        #if both of them all false compute CPG and generate CFG \n",
    "            result =subprocess.call([joern_path+\"./joern-parse\",src_file,\"--out\",out_path+'/'+path_src.split(\"/\")[-1]+\".cpg\"],\n",
    "                        stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL)\n",
    "            result1 =subprocess.call([joern_path+\"/joern-export\",src_file.replace('.c','.cpg'),\"--repr\",\"cfg\",'--out',out_path+'/cfg/'],\n",
    "                        stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL)\n",
    "            \n",
    "print(\"--- %s miuntes ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-imagination",
   "metadata": {},
   "source": [
    "## 2. Compiling CFGs into graph object and storing them in Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_empty  ='digraph .*\\{ *\\n*\\}' #to check if generated CFG is empty or not\n",
    "pd_code      = pd.read_json(dataset_file)\n",
    "\n",
    "pd_list = []\n",
    "for i in project_path:\n",
    "    path = pathlib.PurePath(i)\n",
    "    project_name = path.name\n",
    "    project = glob.glob(i+'/*')\n",
    "    \n",
    "    for path_src in tqdm(project):\n",
    "        new_dict   = dict()\n",
    "        src_file   = str(Path(path_src+'/'+pathlib.PurePath(path_src).name+'.c').resolve())\n",
    "        cfg_folder = str(Path(path_src+'/'+'cfg/').resolve())\n",
    "        index      = int(pathlib.PurePath(src_file).name.replace(\".c\",\"\"))\n",
    "        target     = pd_code.loc[index][\"target\"]\n",
    "        with open(src_file, 'r') as f:\n",
    "            src_code = f.read()\n",
    "        dot_arr = []\n",
    "        for file in glob.glob(cfg_folder+\"/*\"):\n",
    "            with open(file,'r') as f:\n",
    "                dot_arr.append(f.read())\n",
    "        dot_arr = [x for x in dot_arr if not re.search(check_empty, x)] # removes empty graphs\n",
    "        if (len(dot_arr) == 1):                                         #if graph is not empty and is connected\n",
    "            is_connected = True\n",
    "            #G = nx.Graph(nx.drawing.nx_pydot.read_dot(Path(cfg_folder).joinpath(\"0-cfg.dot\")))\n",
    "            with open(Path(cfg_folder).joinpath(\"0-cfg.dot\")) as f:\n",
    "                dotFormat = f.read()\n",
    "            new_str = dotFormat.replace('\\\\\"', '')                        #To catch escape characters\n",
    "            G = nx.drawing.nx_agraph.from_agraph(pygraphviz.AGraph(new_str)) #convert graph into Networkx object\n",
    "        else:\n",
    "            is_connected = False                                       # doesn't check if graph list is empty (update)\n",
    "            G = nx.Graph()\n",
    "             \n",
    "        #creating attributes for the dataset to store them in pandas  \n",
    "        new_dict['index']         = index\n",
    "        new_dict['project']       = project_name\n",
    "        new_dict['target']        = target\n",
    "        new_dict['func_code']     = src_code\n",
    "        new_dict['graph']         = G\n",
    "        new_dict['is_connected']  = is_connected\n",
    "        new_dict['dot_string']    = new_str\n",
    "        pd_list.append(new_dict)\n",
    "        \n",
    "data = pd.DataFrame(pd_list)\n",
    "data.set_index(\"index\",inplace=True, verify_integrity=True)\n",
    "data.to_pickle(\"cfg_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-cornell",
   "metadata": {},
   "source": [
    "### 2.1. Extracting unique operation keywords in the dataset for node embedding\n",
    "- I have extracted all the unique C-tokens that are used in the dataset\n",
    "- I also have extracted all the unique operation words used in the dataset\n",
    "- As you can see I have hardcoded the unique tokens and operations the reasons are:\n",
    "    - There are actually 84 unique tokens in the tokenizer dictionary but in our dataset, there are only 64 unique tokens. That's why I discarded the other tokens to reduce the dimension of node embedding. You can get all the available tokens in the tokenizer by running the code tokenizer_c.tokens\n",
    "    - The reason I have also hardcoded the unique operations is that I didn't want to compute all the operators every time. You can get all the unique operation words by running the following function\n",
    "- To encode the node features (source code) I have done the following:\n",
    "    - I have tokenized the whole code available at the node and selected only unique tokens from it\n",
    "    - I have gotten the operation word at each node can compare it with the type of operations I have. It is possible that a source code contains function calling code and it has no operation type so the operation type will be a zero vector\n",
    "- I have concatenated one-hot encoding of both representation\n",
    "- I have stored the embeddings in a pickle file and I will integrate them into the graphs in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_arr = ['EQUALS',\n",
    "  'LE',\n",
    "  'LNOT',\n",
    "  'RETURN',\n",
    "  'SEMI',\n",
    "  'EQ',\n",
    "  'PLUSPLUS',\n",
    "  'STATIC',\n",
    "  'TIMES',\n",
    "  'ARROW',\n",
    "  'SIZEOF',\n",
    "  'MINUS',\n",
    "  'LOR',\n",
    "  'LAND',\n",
    "  'GE',\n",
    "  'AND',\n",
    "  'OR',\n",
    "  'INT',\n",
    "  'COLON',\n",
    "  'PLUS',\n",
    "  'OREQUAL',\n",
    "  'NE',\n",
    "  'MOD',\n",
    "  'LT',\n",
    "  'CONDOP',\n",
    "  'PERIOD',\n",
    "  'RSHIFT',\n",
    "  'LSHIFT',\n",
    "  'GT',\n",
    "  'NOT',\n",
    "  'ANDEQUAL',\n",
    "  'PLUSEQUAL',\n",
    "  'MINUSEQUAL',\n",
    "  'LONG',\n",
    "  'UNSIGNED',\n",
    "  'TIMESEQUAL',\n",
    "  'CHAR',\n",
    "  'RSHIFTEQUAL',\n",
    "  'DOUBLE',\n",
    "  'MINUSMINUS',\n",
    "  'STRUCT',\n",
    "  'LSHIFTEQUAL',\n",
    "  'IF',\n",
    "  'REGISTER',\n",
    "  'FOR',\n",
    "  'DIVEQUAL',\n",
    "  'XOR',\n",
    "  'AUTO',\n",
    "  'WHILE',\n",
    "  'CONTINUE',\n",
    "  'SHORT',\n",
    "  'XOREQUAL',\n",
    "  'FLOAT',\n",
    "  'DEFAULT',\n",
    "  'ELLIPSIS',\n",
    "  'MODEQUAL',\n",
    "  'UNION',\n",
    "  'DO',\n",
    "  'ENUM',\n",
    "  'BREAK',\n",
    "  'SIGNED',\n",
    "  'ELSE',\n",
    "  'CASE',\n",
    "  'VOLATILE']\n",
    "\n",
    "op_arr = ['RETURN',\n",
    " 'METHOD',\n",
    " 'METHOD_RETURN',\n",
    " '<operator>.indirectFieldAccess',\n",
    " '<operator>.assignment',\n",
    " '<operator>.indirectIndexAccess',\n",
    " '<operator>.fieldAccess',\n",
    " '<operator>.addressOf',\n",
    " '<operator>.addition',\n",
    " '<operator>.lessThan',\n",
    " '<operator>.equals',\n",
    " '<operator>.multiplication',\n",
    " '<operator>.and',\n",
    " '<operator>.logicalNot',\n",
    " '<operator>.postIncrement',\n",
    " '<operator>.subtraction',\n",
    " '<operator>.logicalAnd',\n",
    " '<operator>.minus',\n",
    " '<operator>.notEquals',\n",
    " '<operator>.indirection',\n",
    " '<operator>.cast',\n",
    " '<operator>.arithmeticShiftRight',\n",
    " '<operator>.assignmentPlus',\n",
    " '<operator>.logicalOr',\n",
    " '<operator>.sizeOf',\n",
    " '<operator>.shiftLeft',\n",
    " '<operator>.greaterThan',\n",
    " '<operator>.or',\n",
    " '<operator>.division',\n",
    " '<operator>.conditional',\n",
    " '<operator>.greaterEqualsThan',\n",
    " '<operator>.lessEqualsThan',\n",
    " '<operator>.assignmentMinus',\n",
    " '<operator>.postDecrement',\n",
    " '<operator>.modulo',\n",
    " '<operator>.preIncrement',\n",
    " '<operator>.not',\n",
    " '<operator>.assignmentMultiplication',\n",
    " '<operator>.assignmentDivision',\n",
    " '<operator>.preDecrement',\n",
    " '<operator>.plus']\n",
    "\n",
    "token_enc = OneHotEncoder(handle_unknown='ignore', sparse=False) #encoding tokens using one hot\n",
    "op_enc    = OneHotEncoder(handle_unknown='ignore', sparse=False) #encoding operation types using one hot\n",
    "\n",
    "token_enc.fit(np.array(tok_arr).reshape(-1,1))\n",
    "op_enc.fit(np.array(op_arr).reshape(-1,1))\n",
    "\n",
    "with open('cfg_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "unique_word = {}\n",
    "expr = r'^(.+?),'\n",
    "\n",
    "#these words are for paranthesis or constants like (3,4). If they occur just ignore them\n",
    "ignore_words = ['ICONST','CONST', 'ID', 'LBRACKET', 'RBRACKET', 'LPAREN', 'RPAREN', 'LBRACE', 'RBRACE' ] \n",
    "attr_dict    = {}\n",
    "max_len      = 64*13\n",
    "for idx, i in tqdm(data['dot_string'].items()):\n",
    "    attr_dict[idx] = {}\n",
    "    for line in i.split(\"\\n\"):\n",
    "        if \"label = \" in line:\n",
    "            op_arr       = []\n",
    "            split_line  = line.split(\"label = \")\n",
    "            c           = split_line[1]\n",
    "            node_id     = split_line[0].split('\"')[1]\n",
    "            keyword     = re.search(expr,c).group(0)[2:-1] #operation type\n",
    "            source_code = c.split(',')[1].split(')\" ]')[0]\n",
    "            tokens      = tokenizer_c.run_tokenizer_c(source_code)\n",
    "            tokens       = list(map(str, tokens ))\n",
    "            for tok in tokens:\n",
    "                if any(x in tok for x in ignore_words):\n",
    "                    pass\n",
    "                else:\n",
    "                    op_arr.append(tok.split('(')[1].split(',')[0])\n",
    "\n",
    "            op_arr = list(set(op_arr))\n",
    "            \n",
    "            if len(op_arr)>1:\n",
    "                token_ohe    = token_enc.transform(np.array(op_arr).reshape(-1, 1)).flatten()\n",
    "            else:\n",
    "                token_ohe    = token_enc.transform(np.array(['DEAD']).reshape(1, -1)).flatten() #if token list of source code is empty\n",
    "                \n",
    "            token_ohe    = np.pad(token_ohe, [(0, max_len-len(token_ohe))], mode='constant')\n",
    "            op_ohe       = op_enc.transform(np.array([keyword]).reshape(1, -1)).flatten()\n",
    "            feature_vect = np.concatenate((op_ohe,token_ohe))\n",
    "            attr_dict[idx][node_id]  = feature_vect\n",
    "\n",
    "            \n",
    "with open('node_feature.pkl', 'wb') as f:\n",
    "    pickle.dump(attr_dict,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-assistant",
   "metadata": {},
   "source": [
    "## 2.2 Merging node features into graph\n",
    "- I have merged the one hot encoding embeddings with the graph in following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "involved-editor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('node_feature.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "    \n",
    "with open('cfg_data.pkl', 'rb') as f:\n",
    "    cfg_data = pickle.load(f)\n",
    "    \n",
    "    \n",
    "torch_dict = {}\n",
    "for idx, node_feature in tqdm(data.items()):\n",
    "\n",
    "    torch_dict[idx] = {}\n",
    "    row = cfg_data.loc[idx]\n",
    "    G   = row['graph']\n",
    "    nx.set_node_attributes(G, node_feature, name='x') #setting the one hot encoding for each node in the graph\n",
    "    torch_dict[idx]['graph']        = G\n",
    "    torch_dict[idx]['target']       = row['target']\n",
    "    torch_dict[idx]['project']      = row['project']\n",
    "    torch_dict[idx]['is_connected'] = row['is_connected']\n",
    "\n",
    "df = pd.DataFrame(torch_dict)        #forgot to take transpose of it (update)\n",
    "pd.to_pickle('all_dataset.pkl',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-scene",
   "metadata": {},
   "source": [
    "## 3 Training on both projects using GIN\n",
    "- Please configure the model's parameters before running\n",
    "- You can provide training and testing set ratio\n",
    "- If someone can also confirm the code of GIN is authentic ? I am using from spektral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The whole code except for dataset generation is used from the spektral source code\n",
    "\n",
    "################################################################################\n",
    "# Config\n",
    "################################################################################\n",
    "learning_rate = 1e-3  # Learning rate\n",
    "channels = 128  # Hidden units\n",
    "layers = 3  # GIN layers\n",
    "epochs = 100  # Number of training epochs\n",
    "batch_size = 32  # Batch size\n",
    "es_patience = 10  # Patience for early stopping\n",
    "\n",
    "# Parameters\n",
    "F = 64*13 + 13# Dimension of node features\n",
    "n_out = 2  # Dimension of the target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Build model\n",
    "################################################################################\n",
    "class GIN0(Model):\n",
    "    def __init__(self, channels, n_layers):\n",
    "        super().__init__()\n",
    "        self.conv1 = GINConv(channels, epsilon=0, mlp_hidden=[channels, channels])\n",
    "        self.convs = []\n",
    "        for _ in range(1, n_layers):\n",
    "            self.convs.append(\n",
    "                GINConv(channels, epsilon=0, mlp_hidden=[channels, channels])\n",
    "            )\n",
    "        self.pool = GlobalAvgPool()\n",
    "        self.dense1 = Dense(channels, activation=\"relu\")\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.dense2 = Dense(n_out, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a, i = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        for conv in self.convs:\n",
    "            x = conv([x, a])\n",
    "        x = self.pool([x, i])\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "\n",
    "def evaluate(loader):\n",
    "    output = []\n",
    "    step = 0\n",
    "    while step < loader.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader.__next__()\n",
    "        pred = model(inputs, training=False)\n",
    "        outs = (\n",
    "            loss_fn(target, pred),\n",
    "            tf.reduce_mean(categorical_accuracy(target, pred)),\n",
    "            len(target),  # Keep track of batch size\n",
    "        )\n",
    "        output.append(outs)\n",
    "        if step == loader.steps_per_epoch:\n",
    "            output = np.array(output)\n",
    "            return np.average(output[:, :-1], 0, weights=output[:, -1])\n",
    "\n",
    "    \n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, **kwargs):\n",
    "        self.n_samples = len(data)\n",
    "        self.data     = data\n",
    "        self.unique, self.inverse = np.unique(data['target'].to_numpy(), return_inverse=True)\n",
    "        self.onehot = np.eye(self.unique.shape[0])[self.inverse]\n",
    "        self.zero_array = []\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        def make_graph(idx, row):\n",
    "            G = row['graph'].to_undirected()\n",
    "            \n",
    "            if len(G.nodes())>0:\n",
    "                \n",
    "                # Node features\n",
    "                x = np.array(list(nx.get_node_attributes(G, \"x\").values()))\n",
    "\n",
    "                # Edges\n",
    "                a = nx.adjacency_matrix(G).todense()\n",
    "                a = sp.csr_matrix(a)\n",
    "\n",
    "                # Labels\n",
    "                y = self.onehot[idx] #labels also shoud be one hot encoding\n",
    "\n",
    "                return Graph(x=x, a=a, y=y)\n",
    "            \n",
    "            else:\n",
    "                self.zero_array.append(1)\n",
    "                self.n_samples -= 1\n",
    "                return None\n",
    "                \n",
    "        # We must return a list of Graph. Objects of other types are discarded\n",
    "        return [j for j in [make_graph(i, row) for i, row in self.data.iterrows()] if j is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_spk(data1, train_size = 0.65, val_size = 0.10  ,test_size=0.25, k_fold = 5):\n",
    "    if (train_size = 0.65, val_size = 0.10  ,test_size=0.25) != 1:\n",
    "        print('Wrong test, train, val size. Must be equal to 1')\n",
    "        return\n",
    "    \n",
    "    acc_dict1 = {}\n",
    "    for i in range(k_fold):\n",
    "        idxs = np.random.permutation(len(data1))\n",
    "        split_va, split_te = int(train_size * len(data1)), int((train_size+val_size) * len(data1))\n",
    "        idx_tr, idx_va, idx_te = np.split(idxs, [split_va, split_te])\n",
    "        data_tr = data1[idx_tr]\n",
    "        data_va = data1[idx_va]\n",
    "        data_te = data1[idx_te]\n",
    "\n",
    "        # Data loaders\n",
    "        loader_tr = DisjointLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
    "        loader_va = DisjointLoader(data_va, batch_size=batch_size)\n",
    "        loader_te = DisjointLoader(data_te, batch_size=batch_size)\n",
    "\n",
    "\n",
    "        ################################################################################\n",
    "        # Fit model\n",
    "        ################################################################################\n",
    "        @tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
    "        def train_step(inputs, target):\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(inputs, training=True)\n",
    "                loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            acc = tf.reduce_mean(categorical_accuracy(target, predictions))\n",
    "            return loss, acc\n",
    "\n",
    "        # Build model\n",
    "        model = GIN0(channels, layers)\n",
    "        optimizer = Adam(learning_rate)\n",
    "        loss_fn = CategoricalCrossentropy()\n",
    "        epoch = step = 0\n",
    "        best_val_loss = np.inf\n",
    "        best_weights = None\n",
    "        patience = es_patience\n",
    "        results = []\n",
    "        for batch in loader_tr:\n",
    "            step += 1\n",
    "            loss, acc = train_step(*batch)\n",
    "            results.append((loss, acc))\n",
    "            if step == loader_tr.steps_per_epoch:\n",
    "                step = 0\n",
    "                epoch += 1\n",
    "\n",
    "                # Compute validation loss and accuracy\n",
    "                val_loss, val_acc = evaluate(loader_va)\n",
    "                print(\n",
    "                    \"Ep. {} - Loss: {:.3f} - Acc: {:.3f} - Val loss: {:.3f} - Val acc: {:.3f}\".format(\n",
    "                        epoch, *np.mean(results, 0), val_loss, val_acc\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # Check if loss improved for early stopping\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience = es_patience\n",
    "                    print(\"New best val_loss {:.3f}\".format(val_loss))\n",
    "                    best_weights = model.get_weights()\n",
    "                else:\n",
    "                    patience -= 1\n",
    "                    if patience == 0:\n",
    "                        print(\"Early stopping (best val_loss: {})\".format(best_val_loss))\n",
    "                        break\n",
    "                results = []\n",
    "\n",
    "        ################################################################################\n",
    "        # Evaluate model\n",
    "        ################################################################################\n",
    "        model.set_weights(best_weights)  # Load best model\n",
    "        test_loss, test_acc = evaluate(loader_te)\n",
    "        print(\"Done. Test loss: {:.4f}. Test acc: {:.2f}\".format(test_loss, test_acc))\n",
    "        acc_dict1[i]  = {}\n",
    "        acc_dict1[i]['test_loss'] = test_loss\n",
    "        acc_dict1[i]['test_acc'] = test_acc\n",
    "    return acc_dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving acc in dict:acc one might want to print and calculate average accuracy.\n",
    "# I am working on it. \n",
    "data = pd.read_pickle('all_dataset.pkl')\n",
    "data = data.T\n",
    "data0, data_0 = data.groupby('project')\n",
    "data_ff = data0[1]\n",
    "data_qe = data_0[1]\n",
    "data_ff.index = list(range(len(data_ff)))\n",
    "data_qe.index = list(range(len(data_qe)))\n",
    "del data\n",
    "acc_qe = train_spk(data_qe)\n",
    "acc_ff = train_spk(data_ff)\n",
    "acc = {\"qemu\"acc_qe, \"FFmpeg\": acc_ff}\n",
    "with open('acc_results.pkl') as f:\n",
    "    pickle.dump(acc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-shell",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
