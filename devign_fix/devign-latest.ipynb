{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the dataset (using cfg_pkl, our dataset)\n",
    "- Split it into projects (ffmpeg and qemu in our case)\n",
    "- tokenize the source code using their tokenizer \n",
    "- Generate word2vec representation using their trained model using CFG node level information\n",
    "- convert the data into torch_geometric format\n",
    "- train the data on their model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important things\n",
    "- Their source code is using AST for classification\n",
    "- The word2vec model is different because because we are using CFG based node information rather than using AST\n",
    "- I am using their defined hyperparamters for word2vec and their model\n",
    "- Their adacency encoding graph representation is kind of weird. I mean first of all it is disconnected. They have multiple isolated nodes and values of all of their adjacency matrix is 2 instead of being one (ignoring multi-edge graph).\n",
    "- Their is some error while converting graph to their custom representation. I was not able to convert some of their instances from torch_geometric to simple adjacencey matrix.\n",
    "- Slightly irrelvevant, There are some issues as well in torch geomteric. It save the edge list and edge wait (sum of edges in our case) in another attribute called edge_attr and cant be used during calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anwar/anaconda3/envs/my_env/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import configs, gc\n",
    "from tabulate import tabulate\n",
    "from halo import HaloNotebook as Halo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import src.data as data_util\n",
    "import src.process as process\n",
    "from src.utils.functions.parse import tokenizer\n",
    "import torch\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import convert\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "class NodesEmbedding:\n",
    "    def __init__(self, nodes_dim: int, w2v_keyed_vectors: Word2VecKeyedVectors):\n",
    "        self.w2v_keyed_vectors = w2v_keyed_vectors\n",
    "        self.kv_size = w2v_keyed_vectors.vector_size\n",
    "        self.nodes_dim = nodes_dim\n",
    "\n",
    "        assert self.nodes_dim >= 0\n",
    "\n",
    "        # Buffer for embeddings with padding\n",
    "        self.target = torch.zeros(self.nodes_dim, self.kv_size).float()\n",
    "\n",
    "    def __call__(self, nodes):\n",
    "        embedded_nodes = self.embed_nodes(nodes)\n",
    "\n",
    "        nodes_tensor = torch.from_numpy(embedded_nodes).float()\n",
    "\n",
    "        self.target[:nodes_tensor.size(0), :] = nodes_tensor\n",
    "\n",
    "        return self.target\n",
    "\n",
    "    def embed_nodes(self, G):\n",
    "        embeddings = []\n",
    "\n",
    "        for (n,d) in G.nodes(data=True):\n",
    "            # Get node's code\n",
    "            node_code = d\n",
    "            # Tokenize the code\n",
    "            tokenized_code = tokenizer(\"\".join(d.values()))\n",
    "            if not tokenized_code:\n",
    "                # print(f\"Dropped node {node}: tokenized code is empty.\")\n",
    "                msg = f\"Empty TOKENIZED from node CODE {node_code}\"\n",
    "                print(msg)\n",
    "            # Get each token's learned embedding vector\n",
    "            vectorized_code = np.array(self.get_vectors(tokenized_code))\n",
    "            # The node's source embedding is the average of it's embedded tokens\n",
    "            source_embedding = np.mean(vectorized_code, 0)\n",
    "            # The node representation is the concatenation of label and source embeddings\n",
    "            #embedding = np.concatenate((np.array([node.type]), source_embedding), axis=0)\n",
    "            embeddings.append(source_embedding)\n",
    "        # print(node.label, node.properties.properties.get(\"METHOD_FULL_NAME\"))\n",
    "\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    # fromTokenToVectors\n",
    "    def get_vectors(self, tokenized_code):\n",
    "        vectors = []\n",
    "        for token in tokenized_code:\n",
    "            if token in self.w2v_keyed_vectors.key_to_index:\n",
    "                vectors.append(self.w2v_keyed_vectors[token])\n",
    "            else:\n",
    "                # print(node.label, token, node.get_code(), tokenized_code)\n",
    "                vectors.append(np.zeros(self.kv_size))\n",
    "        return vectors\n",
    "\n",
    "\n",
    "\n",
    "def nodes_to_input(G, target, nodes_dim, keyed_vectors):\n",
    "    nodes_embedding = NodesEmbedding(nodes_dim, keyed_vectors)\n",
    "    edge_index, edge_attr = convert.from_scipy_sparse_matrix(nx.adjacency_matrix(G))\n",
    "    label = torch.tensor([target]).float()\n",
    "\n",
    "    return Data(x=nodes_embedding(G), edge_index=edge_index, edge_attr=edge_attr ,y=label)\n",
    "\n",
    "\n",
    "\n",
    "def process_task(stopping, cpg_dataset):\n",
    "    context = configs.Process()\n",
    "    devign = configs.Devign()\n",
    "    model_path = PATHS.model + FILES.model\n",
    "    model = process.Devign(path=model_path, device=DEVICE, model=devign.model, learning_rate=devign.learning_rate,\n",
    "                           weight_decay=devign.weight_decay,\n",
    "                           loss_lambda=devign.loss_lambda)\n",
    "    train = process.Train(model, context.epochs)\n",
    "    input_dataset = cpg_dataset\n",
    "    # split the dataset and pass to DataLoader with batch size\n",
    "    train_loader, val_loader, test_loader = data_util.train_val_test_split(input_dataset, shuffle=context.shuffle)\n",
    "    train_loader_step = process.LoaderStep(\"Train\", train_loader, DEVICE)\n",
    "    val_loader_step = process.LoaderStep(\"Validation\", val_loader, DEVICE)\n",
    "    test_loader_step = process.LoaderStep(\"Test\", test_loader, DEVICE)\n",
    "\n",
    "    if stopping:\n",
    "        early_stopping = process.EarlyStopping(model, patience=context.patience)\n",
    "        train(train_loader_step, val_loader_step, early_stopping)\n",
    "        model.load()\n",
    "    else:\n",
    "        train(train_loader_step, val_loader_step)\n",
    "        model.save()\n",
    "\n",
    "    process.predict(model, test_loader_step)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change word2vec hyperparamters and max nodes dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_size  = 150\n",
    "nodes_dim = int(group.apply(lambda g: nx.number_of_nodes(g.graph),axis=1).describe()['max'])\n",
    "\n",
    "with open('config.json') as f:\n",
    "    json_config = json.load(f)\n",
    "    \n",
    "json_config['devign']['model']['conv_args']['conv1d_1']['in_channels'] = nodes_dim\n",
    "json_config['embed']['nodes_dim']  = nodes_dim\n",
    "json_config['devign']['model']['emb_size']  = w2v_size\n",
    "json_config['embed']['word2vec_args']['size']  = w2v_size\n",
    "\n",
    "with open('config.json', 'w') as f:\n",
    "    json.dump(json_config,f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS = configs.Paths()\n",
    "FILES = configs.Files()\n",
    "DEVICE = FILES.get_device()\n",
    "\n",
    "context = configs.Process()\n",
    "devign = configs.Devign()\n",
    "model_path = PATHS.model + FILES.model\n",
    "context = configs.Embed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('../cfg_data.pkl')\n",
    "data = data[(data['is_connected']==True) & (np.array([G.number_of_nodes()>0 for G in data['graph'].values]))]\n",
    "data = data[['target', 'project', 'graph','func_code']]\n",
    "data = data.rename(columns={'func_code': 'func'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenzing code, training word2vec, changing graph representation to torch_geometric and training devign model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " **************************************** \n",
      "\n",
      "No of samples in dataset FFmpeg: 9576 \n",
      "+------+--------------------------+\n",
      "|      | Node stats for: FFmpeg   |\n",
      "+======+==========================+\n",
      "| min  | 2                        |\n",
      "+------+--------------------------+\n",
      "| max  | 2659                     |\n",
      "+------+--------------------------+\n",
      "| mean | 146.661                  |\n",
      "+------+--------------------------+\n",
      "| std  | 209.337                  |\n",
      "+------+--------------------------+ \n",
      "\n",
      "+------+--------------------------+\n",
      "|      | Edge stats for: FFmpeg   |\n",
      "+======+==========================+\n",
      "| min  | 1                        |\n",
      "+------+--------------------------+\n",
      "| max  | 2864                     |\n",
      "+------+--------------------------+\n",
      "| mean | 162.551                  |\n",
      "+------+--------------------------+\n",
      "| std  | 234.159                  |\n",
      "+------+--------------------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097fa25fb1514d57a4f11eacdbfbc2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving input dataset FFmpeg with size 9576.\n",
      "Saving w2vmodel.\n",
      "new fc1 1500 new fc2 size 1000\n",
      "The model has 1,098,872 trainable parameters\n",
      "Splitting Dataset\n",
      "\n",
      "Epoch 1; - Train Loss: 0.2138; Acc: 0.1778; - Validation Loss: 4.4038; Acc: 0.13; - Time: 978.7943940162659\n",
      "\n",
      "Epoch 2; - Train Loss: 0.2605; Acc: 0.1565; - Validation Loss: 3.8768; Acc: 0.13; - Time: 1942.0359108448029\n",
      "\n",
      "Epoch 3; - Train Loss: 0.2813; Acc: 0.179; - Validation Loss: 3.553; Acc: 0.13; - Time: 2905.7445142269135\n",
      "\n",
      "Epoch 4; - Train Loss: 0.2762; Acc: 0.1778; - Validation Loss: 3.5249; Acc: 0.13; - Time: 3869.289839744568\n",
      "\n",
      "Epoch 5; - Train Loss: 0.3602; Acc: 0.1885; - Validation Loss: 3.3769; Acc: 0.1379; - Time: 4835.737769842148\n",
      "\n",
      "Epoch 6; - Train Loss: 0.4419; Acc: 0.1636; - Validation Loss: 3.4929; Acc: 0.1379; - Time: 5799.603452682495\n",
      "\n",
      "Epoch 7; - Train Loss: 0.3888; Acc: 0.1802; - Validation Loss: 3.2758; Acc: 0.13; - Time: 6767.889722108841\n",
      "\n",
      "Epoch 8; - Train Loss: 0.3936; Acc: 0.1923; - Validation Loss: 3.5199; Acc: 0.13; - Time: 7732.048662185669\n",
      "\n",
      "Epoch 9; - Train Loss: 0.4629; Acc: 0.1671; - Validation Loss: 3.1336; Acc: 0.13; - Time: 8701.335347414017\n",
      "\n",
      "Epoch 10; - Train Loss: 0.4146; Acc: 0.192; - Validation Loss: 3.1568; Acc: 0.1379; - Time: 9667.459969043732\n",
      "\n",
      "Epoch 11; - Train Loss: 0.4065; Acc: 0.2064; - Validation Loss: 3.4339; Acc: 0.1379; - Time: 10633.535828351974\n",
      "\n",
      "Epoch 12; - Train Loss: 0.4486; Acc: 0.1897; - Validation Loss: 3.3094; Acc: 0.1379; - Time: 11639.817940950394\n"
     ]
    }
   ],
   "source": [
    "for name, group in data.groupby('project'):\n",
    "    \n",
    "    print('\\n'*3,\"*\"*40,'\\n')\n",
    "    node_size_group = group.apply(lambda g: nx.number_of_nodes(g.graph),axis=1).describe()[['min', 'max','mean','std']]\n",
    "    print(\"No of samples in dataset {}: {} \".format(name, len(group)))\n",
    "    print(tabulate(node_size_group .to_frame(),\n",
    "                   tablefmt=\"grid\", stralign='left', numalign='left',\n",
    "                   headers=['Node stats for: {}'.format(name)]) ,'\\n')\n",
    "    \n",
    "    nodes_dim = int(node_size_group['max'])\n",
    "    \n",
    "    edge_size_group = group.apply(lambda g: nx.number_of_edges(g.graph),axis=1).describe()[['min', 'max','mean','std']]\n",
    "    print(tabulate(edge_size_group.to_frame(),\n",
    "                   tablefmt=\"grid\", stralign='left', numalign='left',\n",
    "                   headers=['Edge stats for: {}'.format(name)]))\n",
    "    \n",
    "    spinner = Halo(text='Tokenizing source code', spinner='dots')\n",
    "    spinner.start()\n",
    "    tokens_dataset = data_util.tokenize(group)\n",
    "    spinner.stop()\n",
    "    spinner.clear()\n",
    "    \n",
    "    #you can change here that instead of training word2vec again load the available dataset\n",
    "    spinner = Halo(text='Training word2vec on tokens code', spinner='dots')\n",
    "    w2vmodel = Word2Vec(**context.w2v_args)\n",
    "    w2vmodel.build_vocab(tokens_dataset.tokens)\n",
    "    w2vmodel.train(tokens_dataset.tokens, total_examples=w2vmodel.corpus_count, epochs=1)\n",
    "    spinner.stop()\n",
    "    \n",
    "    spinner = Halo(text='Converting tokens to word2vec representation', spinner='dots')\n",
    "    group[\"input\"] = group.apply(lambda row: nodes_to_input(row.graph, row.target, nodes_dim,\n",
    "                                                                                    w2vmodel.wv), axis=1)\n",
    "    spinner.stop()\n",
    "    spinner.clear()\n",
    "    \n",
    "    print(f\"Saving input dataset {name} with size {len(group)}.\")\n",
    "    pd.to_pickle(group[[\"input\", \"target\"]], f'data/input/{name}.pkl')\n",
    "    gc.collect()\n",
    "    print(\"Saving w2vmodel.\")\n",
    "    w2vmodel.save(f\"{PATHS.w2v}/{name}+word2vec.model\")\n",
    "    #print('Training for {}'.format(name))\n",
    "    spinner = Halo(text='Training and Testing for {}'.format(name), spinner='dots')\n",
    "    stopping = False\n",
    "    context = configs.Process()\n",
    "    devign = configs.Devign()\n",
    "    model_path = PATHS.model + FILES.model\n",
    "    model = process.Devign(path=model_path, device=DEVICE, model=devign.model, learning_rate=devign.learning_rate,\n",
    "                           weight_decay=devign.weight_decay,\n",
    "                           loss_lambda=devign.loss_lambda)\n",
    "    train = process.Train(model, context.epochs)\n",
    "    input_dataset = group[[\"input\", \"target\"]]\n",
    "    # split the dataset and pass to DataLoader with batch size\n",
    "    train_loader, val_loader, test_loader = list(\n",
    "        map(lambda x: x.get_loader(context.batch_size, shuffle=context.shuffle),\n",
    "            data_util.train_val_test_split(input_dataset, shuffle=context.shuffle)))\n",
    "    train_loader_step = process.LoaderStep(\"Train\", train_loader, DEVICE)\n",
    "    val_loader_step = process.LoaderStep(\"Validation\", val_loader, DEVICE)\n",
    "    train(train_loader_step, val_loader_step)\n",
    "    print('Finish Training for {}'.format(name))\n",
    "    spinner.stop()\n",
    "    spinner.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
