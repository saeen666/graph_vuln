{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1.0 - The first task takes the given dataset and train a word2vec model on it "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import pathlib, glob, time, subprocess, re, pygraphviz, json\r\n",
    "from tabulate import tabulate\r\n",
    "import pandas as pd\r\n",
    "from pathlib import Path\r\n",
    "from tqdm.auto import tqdm\r\n",
    "import networkx as nx\r\n",
    "import scipy.sparse as sp\r\n",
    "import numpy as np\r\n",
    "from src.utils.functions.parse import tokenizer\r\n",
    "import src.data as data_util\r\n",
    "from gensim.models.word2vec import Word2Vec"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#location to csv file, in our case it is MSR_data\r\n",
    "new_data  = pd.read_csv('msr/MSR_data_cleaned.csv')\r\n",
    "new_data_source = pd.DataFrame(data={'func':pd.concat([new_data['func_before'], new_data['func_after']])}) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ammar/vuln_code/vuln_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (20,22,23,27,28,29) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "tokens_dataset = data_util.tokenize(new_data_source.iloc[0:100])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ammar/vuln_code/vuln_env/lib/python3.9/site-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Tokenizing source code')\r\n",
    "tokens_dataset = data_util.tokenize(new_data_source)\r\n",
    "\r\n",
    "#you can change here that instead of training word2vec again load the available dataset\r\n",
    "print('Training word2vec on tokens')\r\n",
    "w2vmodel = Word2Vec(sentences=tokens_dataset.tokens, vector_size=200, window=5, min_count=1, workers=4, epochs=10)\r\n",
    "print('Saving word2vec model')\r\n",
    "w2vmodel.save('msr/all_w2v_model_devign_tokens.model')\r\n",
    "#load it using: Word2Vec.load('msr/all_w2v_model_devign_tokens.model')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sample_size = 11000\r\n",
    "new_data_pos = new_data[new_data['vul']==1]['func_before']\r\n",
    "new_data_neg = new_data[new_data['vul']==0]['func_after'].sample(sample_size)\r\n",
    "\r\n",
    "from pathlib import Path\r\n",
    "curr_path = \"new_dataset_update/positive\" \r\n",
    "Path(curr_path).mkdir(parents=True, exist_ok=True)\r\n",
    "curr_path = \"new_dataset_update/negative\"\r\n",
    "Path(curr_path).mkdir(parents=True, exist_ok=True)\r\n",
    "\r\n",
    "#writing the code from pandas to individual .c files\r\n",
    "for key, c_code in new_data_pos.items():\r\n",
    "    with open('new_dataset_update/positive/{}.c'.format(key), 'w') as f:\r\n",
    "        f.write(c_code)\r\n",
    "        \r\n",
    "for key, c_code in new_data_neg.items():\r\n",
    "    with open('new_dataset_update/negative/{}.c'.format(key), 'w') as f:\r\n",
    "        f.write(c_code)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.0 - The following code convert the C-files in the given directory to cfg representation "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# change the given three variable according to your environment\r\n",
    "graph_type = 'cfg'\r\n",
    "path_code = pathlib.Path('new_dataset_update/')   #where c code is present in the folder                    \r\n",
    "joern_path = '/home/ammar/vuln_code/joern-cli/'   #local path where joern-cli is present\r\n",
    "\r\n",
    "# iterate over each group/project\r\n",
    "for folder in tqdm(glob.glob('{}/*'.format(path_code))): #the group/folders in the directory are\r\n",
    "                                                            #negative and positive in this case\r\n",
    "    if graph_type not in folder and Path(folder).is_dir():\r\n",
    "        print(folder)\r\n",
    "        path_group = pathlib.Path('{}_{}'.format(folder,graph_type))         #make folder for the dataset\r\n",
    "        path_group.mkdir(parents=True, exist_ok=True)\r\n",
    "    \r\n",
    "        start_time = time.time()\r\n",
    "\r\n",
    "        project = glob.glob(folder+'/*')                  #get all .c files we just generated\r\n",
    "\r\n",
    "        for path_src in tqdm(project):\r\n",
    "            src_file      = str(Path(path_src).resolve())\r\n",
    "            out_path      = str(Path(str(path_group)+'/'+path_src.split('/')[-1].replace('.c','')).resolve())+\"/\"\r\n",
    "            file_dir      = pathlib.Path(str(Path(str(path_group)+'/'+path_src.split('/')[-1].replace('.c','')).resolve())+\"/\")         #make folder for the dataset\r\n",
    "            file_dir.mkdir(parents=True, exist_ok=True)\r\n",
    "\r\n",
    "            cpg_path_cmd  = [joern_path+\"./joern-parse\",src_file,\"--out\",out_path+src_file.split(\"/\")[-1].replace(\".c\",\"\")+\".cpg\"]\r\n",
    "            graph_out_cmd = [joern_path+\"joern-export\",out_path+src_file.split(\"/\")[-1].replace(\".c\",\"\")+'.cpg',\"--repr\",graph_type,'--out',out_path+'/cfg/']\r\n",
    "            cpg_check     = Path(out_path+src_file.split(\"/\")[-1].replace(\".c\",\"\")+\".cpg\").is_file()\r\n",
    "            \r\n",
    "            pth = pathlib.Path(str(Path(str(path_group)+'/'+path_src.split('/')[-1].replace('.c','')).resolve())+\"/\")\r\n",
    "            if cpg_check and not Path(out_path+'cfg/').is_dir():\r\n",
    "                result = subprocess.call(graph_out_cmd,stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL)\r\n",
    "\r\n",
    "            # check if CFG and both CPG are present if true nothing will be generated\r\n",
    "            elif cpg_check and Path(out_path+'cfg/').is_dir():\r\n",
    "                pass\r\n",
    "\r\n",
    "            else:\r\n",
    "            #if both of them all false compute CPG and generate CFG \r\n",
    "                result = subprocess.call(cpg_path_cmd,stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL)\r\n",
    "\r\n",
    "                result1 = subprocess.call(graph_out_cmd, stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL)\r\n",
    "                \r\n",
    "            with open(out_path+pth.name+'.c', 'w') as f, open(src_file, 'r') as f1:\r\n",
    "                f.write(f1.read())\r\n",
    "\r\n",
    "        print(\"--- %s miuntes ---\" % ((time.time() - start_time)/60))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.0 - Converting the CFG representation into networkx graph "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "check_empty  ='digraph .*\\{ *\\n*\\}' #to check if generated CFG is empty or not\r\n",
    "pd_list = []\r\n",
    "dict_e = {}\r\n",
    "path_code = 'new_dataset_update' #I have changed the name\r\n",
    "project_path = glob.glob(path_code+\"/*_cfg\")\r\n",
    "for i in tqdm(project_path):\r\n",
    "    path = pathlib.PurePath(i)\r\n",
    "    project_name = path.name \r\n",
    "    project = glob.glob(i+'/*')\r\n",
    "    dict_e[i+' total'] = len(project)\r\n",
    "    dict_e[i+' removed'] = 0\r\n",
    "    for path_src in tqdm(project, 'total files'):\r\n",
    "        #print(path_src)\r\n",
    "        new_dict   = dict()\r\n",
    "        src_file   = str(Path(path_src+'/'+pathlib.PurePath(path_src).name+'.c').resolve())\r\n",
    "        cfg_folder = str(Path(path_src+'/'+'cfg/').resolve())\r\n",
    "        index      = int(pathlib.PurePath(src_file).name.replace(\".c\",\"\"))\r\n",
    "        target     = project_name.split('_')[0]\r\n",
    "        #src_file   =  str(Path(os.path.join('', *[path_code,target,str(index)+'.c'])).resolve()) #changed for new_Dataset\r\n",
    "        #print('{}- {}- {}- {}- {}'.format(path_src, cfg_folder,index,target,src_file))\r\n",
    "        with open(src_file, 'r') as f:\r\n",
    "            src_code = f.read()\r\n",
    "        dot_arr = []\r\n",
    "        for file in glob.glob(cfg_folder+\"/*\"):\r\n",
    "            with open(file,'r') as f:\r\n",
    "                dot_arr.append(f.read())\r\n",
    "        dot_arr = [x for x in dot_arr if not re.search(check_empty, x)] # removes empty graphs\r\n",
    "        if (len(dot_arr) == 1):                                         #if graph is not empty and is connected\r\n",
    "            is_connected = True\r\n",
    "            #G = nx.Graph(nx.drawing.nx_pydot.read_dot(Path(cfg_folder).joinpath(\"0-cfg.dot\")))\r\n",
    "            try:\r\n",
    "                with open(Path(cfg_folder).joinpath(\"0-cfg.dot\")) as f:\r\n",
    "                    dotFormat = f.read()\r\n",
    "                new_str = dotFormat.replace('\\\\\"', '')                       #To catch escape characters\r\n",
    "                new_str = \"\\n\".join([f_str.strip() for f_str in dotFormat.split('\\n')])\r\n",
    "                G = nx.drawing.nx_agraph.from_agraph(pygraphviz.AGraph(new_str)); #convert graph into Networkx object\r\n",
    "                new_dict['index']         = index\r\n",
    "                new_dict['project']       = project_name\r\n",
    "                new_dict['func_code']     = src_code\r\n",
    "                new_dict['graph']         = G\r\n",
    "                new_dict['is_connected']  = is_connected\r\n",
    "                new_dict['dot_string']    = new_str\r\n",
    "                if target=='positive':\r\n",
    "                    new_dict['target']        = 1\r\n",
    "                elif target =='negative':\r\n",
    "                    new_dict['target']        = 0\r\n",
    "                else:\r\n",
    "                    print('class other than negative and positive found')\r\n",
    "                pd_list.append(new_dict)\r\n",
    "            except ValueError as e:\r\n",
    "                dict_e[i+' removed'] += 1\r\n",
    "                pass\r\n",
    "            \r\n",
    "        dict_e[i+' used'] = dict_e[i+' total'] - dict_e[i+' removed']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 - Displays basic stats of the graph dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(json.dumps(dict_e, indent=4))\r\n",
    "data = pd.DataFrame(pd_list)\r\n",
    "data = data[['target', 'project', 'graph','func_code','index']]\r\n",
    "data = data.rename(columns={'func_code': 'func'})\r\n",
    "print('tokenizing code')\r\n",
    "w2vmodel = Word2Vec.load('new_dataset_update/word2vec.model')\r\n",
    "\r\n",
    "print(\"No of samples in dataset: {} \".format(len(data)))\r\n",
    "print('Complete dataset stats')\r\n",
    "print('\\n'*3,'Complete dataset stats\\n',\"*\"*40,'\\n')\r\n",
    "\r\n",
    "node_size_group = data.apply(lambda g: nx.number_of_nodes(g.graph),axis=1).describe()[['min', 'max','mean','std']]\r\n",
    "\r\n",
    "print(tabulate(node_size_group.to_frame(),\r\n",
    "               tablefmt=\"grid\", stralign='left', numalign='left',\r\n",
    "               headers=['Node stats']))\r\n",
    "\r\n",
    "\r\n",
    "print('\\n'*3)\r\n",
    "\r\n",
    "edge_size_group = data.apply(lambda g: nx.number_of_edges(g.graph),axis=1).describe()[['min', 'max','mean','std']]\r\n",
    "print(tabulate(edge_size_group.to_frame(),\r\n",
    "                   tablefmt=\"grid\", stralign='left', numalign='left',\r\n",
    "                   headers=['Edge stats']))\r\n",
    "\r\n",
    "print('\\n\\n')\r\n",
    "print('Stats for each class')\r\n",
    "print(\"1 = Vulnerable, 0 = Not Vulnerable\")\r\n",
    "print('\\n')\r\n",
    "for name, group in data.groupby('target'):\r\n",
    "    \r\n",
    "    edge_size_group = group.apply(lambda g: nx.number_of_edges(g.graph),axis=1).describe()[['min', 'max','mean','std']]\r\n",
    "    node_size_group = group.apply(lambda g: nx.number_of_nodes(g.graph),axis=1).describe()[['min', 'max','mean','std']]\r\n",
    "    \r\n",
    "    print(tabulate(node_size_group.to_frame(),\r\n",
    "               tablefmt=\"grid\", stralign='left', numalign='left',\r\n",
    "               headers=['Class {} Node stats'.format(name)]))\r\n",
    "    \r\n",
    "    print('\\n'*3)\r\n",
    "    print(tabulate(edge_size_group.to_frame(),\r\n",
    "                   tablefmt=\"grid\", stralign='left', numalign='left',\r\n",
    "                   headers=['Class {} Edge stats'.format(name)]))\r\n",
    "    print('\\n'*3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 - Saves the dataset in pytorch geometric"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data[\"input\"] = data.apply(lambda row: nodes_to_input(row.graph, row.target, nx.number_of_nodes(row.graph),\r\n",
    "                                                                                    w2vmodel.wv), axis=1)\r\n",
    "print('Writing to file/pandas')\r\n",
    "pd.to_pickle(data[['input','target']], 'new_dataset_update/data_random.pkl')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}